{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define input folder\n",
    "input_folder_path = r\"100notes_txt\"\n",
    "\n",
    "# dflow: data flow that passes through the pipeline\n",
    "# dflow has filenames as keys; \n",
    "# values start with the original text, and are updated \n",
    "# to a list that includes results from each pipe\n",
    "dflow = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling model to Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling e796792eba26... 100% ▕████████████████▏  17 GB                         \u001b[K\n",
      "pulling e0a42594d802... 100% ▕████████████████▏  358 B                         \u001b[K\n",
      "pulling dd084c7d92a3... 100% ▕████████████████▏ 8.4 KB                         \u001b[K\n",
      "pulling 3116c5225075... 100% ▕████████████████▏   77 B                         \u001b[K\n",
      "pulling f838f048d368... 100% ▕████████████████▏  490 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling 667b0c1932bc... 100% ▕████████████████▏ 4.9 GB                         \u001b[K\n",
      "pulling 948af2743fc7... 100% ▕████████████████▏ 1.5 KB                         \u001b[K\n",
      "pulling 0ba8f0e314b4... 100% ▕████████████████▏  12 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5... 100% ▕████████████████▏   96 B                         \u001b[K\n",
      "pulling 455f34728c9b... 100% ▕████████████████▏  487 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling 1fa8532d986d... 100% ▕████████████████▏  15 GB                         \u001b[K\n",
      "pulling 6db27cd4e277... 100% ▕████████████████▏  695 B                         \u001b[K\n",
      "pulling 70a4dab5e1d1... 100% ▕████████████████▏ 1.5 KB                         \u001b[K\n",
      "pulling a00920c28dfd... 100% ▕████████████████▏   17 B                         \u001b[K\n",
      "pulling 9b6ac0d4e97e... 100% ▕████████████████▏  494 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!ollama pull gemma3:27b\n",
    "!ollama pull llama3.1:8b\n",
    "!ollama pull mistral-small3.1:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Each time you will run the whole pipeline with one LLM. Follow the instruction below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Change the first line of 'ros_extract' and 'ros_classify' file with the model name <br> - For example, if you run gemma3:27b, change the first line to: <br>FROM gemma3:27b <br> - Similar with other models:<br>FROM llama3.1:8b <br> FROM mistral-small3.1:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you change the files content, run create_model.sh to create ros_extract and ros_classify models based on the corresponding LLM you want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components \u001b[K\n",
      "using existing layer sha256:1fa8532d986d729117d6b5ac2c884824d0717c9468094554fd1d36412c740cfc \u001b[K\n",
      "using existing layer sha256:6db27cd4e277c91264572b9c899c1980daa8dea11e902f0070a6f4763f3d13c8 \u001b[K\n",
      "using existing layer sha256:78f1631bbae9c1601f7ef958c1a2ded966766a1601ca3a4f8b29d0a0c2c60222 \u001b[K\n",
      "using existing layer sha256:77666e5fb92b1205d96fa2c71c8cadfbf56d2ad82636313603ec0113accfd1e1 \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components \u001b[K\n",
      "using existing layer sha256:1fa8532d986d729117d6b5ac2c884824d0717c9468094554fd1d36412c740cfc \u001b[K\n",
      "using existing layer sha256:6db27cd4e277c91264572b9c899c1980daa8dea11e902f0070a6f4763f3d13c8 \u001b[K\n",
      "using existing layer sha256:e112f3568ab826a6f1f317c70809ca002dad47947352d337c7b2b1bbf995df28 \u001b[K\n",
      "using existing layer sha256:77666e5fb92b1205d96fa2c71c8cadfbf56d2ad82636313603ec0113accfd1e1 \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!source create_model.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the output folder to corresponding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_folder_path = r\"Output_gemma3_27b\"\n",
    "output_folder_path = r\"Output_mistral-small3.1\"\n",
    "# output_folder_path = r\"Output_llama3.1_8b\"\n",
    "\n",
    "if os.path.exists(output_folder_path) == False:\n",
    "    os.makedirs(output_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipe 1: Load notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(input_folder_path):\n",
    "    if filename.endswith('.txt'):  # Process only .txt files\n",
    "        file_path = os.path.join(input_folder_path, filename)\n",
    "        with open(file_path) as f:\n",
    "            dflow[filename]=f.read()\n",
    "\n",
    "print(f'{len(dflow)} notes loaded')\n",
    "dflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipe 2: Segmentation to extract the ROS section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Functions Required for ROS Segmentation\"\"\"\n",
    "#====================================================================\n",
    "import pandas as pd\n",
    "def sectag_to_regex(header_file_path, seg_col, header_col):\n",
    "  header_df = pd.read_csv(header_file_path)\n",
    "  header_df = header_df.drop_duplicates()\n",
    "  headers = header_df[header_col].tolist()\n",
    "  header_patterns = [f'^{header}[\\n:]' for header in headers]\n",
    "  return header_patterns, header_df[seg_col].tolist()\n",
    "\n",
    "#====================================================================\n",
    "import re\n",
    "def find_segs(note, header_patterns, seg_names):\n",
    "  segs = {}\n",
    "\n",
    "  # Find the section headers and their start positions\n",
    "  for i, pattern in enumerate(header_patterns):\n",
    "    for m in re.finditer(pattern, note.lower(), re.MULTILINE):\n",
    "      seg_head = (note[m.span()[0]:m.span()[1]], m.span()[0])\n",
    "      if seg_head not in segs:\n",
    "        segs[seg_head] = []  # A seg head can have multiple general seg names\n",
    "\n",
    "      segs[seg_head].append(seg_names[i]) \n",
    "\n",
    "  segs = [[k[0], segs[k], k[1]] for k in segs.keys()]\n",
    "  segs = sorted(segs, key=lambda x: x[2])\n",
    "  \n",
    "  # Find the entir sections and their start and end positions\n",
    "  for i in range(len(segs)):\n",
    "    if i == len(segs)-1:\n",
    "      segs[i].append(len(note))\n",
    "    else:\n",
    "      segs[i].append(segs[i+1][2])\n",
    "\n",
    "  return segs\n",
    "\n",
    "#====================================================================\n",
    "def ros_seg(note, segs):\n",
    "  ros_data = []   # ros text + start position\n",
    "  right_after_ros = False # flag sections after ROS\n",
    "  for seg in segs:\n",
    "      section_names = seg[1]\n",
    "      section_content = note[seg[2]:seg[3]]  \n",
    "\n",
    "      #---------------------------------------------------------------\n",
    "      # Handle hiearchical subsections within ROS if exist\n",
    "      if right_after_ros:\n",
    "        if any(\"review\" in item for item in section_names):\n",
    "          # If there is a review section, append it to the ros_data\n",
    "          ros_data[0] += section_content\n",
    "        else:\n",
    "          right_after_ros = False  # Consider it goes to another section\n",
    "     \n",
    "      #---------------------------------------------------------------\n",
    "      if 'review_of_systems' in section_names:\n",
    "          ros_data = [section_content, seg[2]]\n",
    "          right_after_ros = True\n",
    "\n",
    "  return ros_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_patterns, seg_names = sectag_to_regex(r'SecTag.csv', 'kmname', 'str')\n",
    "for filename, note in dflow.items():\n",
    "    segs = find_segs(note, header_patterns, seg_names)\n",
    "    ros_data = ros_seg(note, segs)        \n",
    "    dflow[filename] = ros_data\n",
    "\n",
    "#print(segs)\n",
    "dflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipe 3: LLM to extract diseases, symptoms and body systems and their positive/negative status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to remove unnescessary characters in json output from LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_char_json(text):\n",
    "    pattern = r'\\[.*\\]'\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    result = match.group()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract diseases and convert to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate([\n",
    "    (\"user\", \"\"\"{ros_text}\"\"\"),\n",
    "    (\"user\", \"convert to json, remove any unnecessary text and make sure the output starts with [\")\n",
    "])\n",
    "\n",
    "llm1=ChatOllama(\n",
    "    model=\"ros_extract\"\n",
    ")\n",
    "\n",
    "chain = prompt|llm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "for filename in dflow.keys():\n",
    "    if dflow[filename] != []:\n",
    "        ros_text = dflow[filename][0]\n",
    "        ros_extract = chain.invoke(ros_text)\n",
    "        \n",
    "        try:\n",
    "            ros_extract_json = json.loads(ros_extract.content)\n",
    "        except:\n",
    "            try:\n",
    "                # Try again, assuming the error was due to the AI output being not a JSON\n",
    "                ros_extract2 = llm1.invoke(f\"Convert to json, remove any unnecessary text, for example ```, and make sure the output starts with [: {ros_extract.content}\")\n",
    "                ros_extract2_corrected = remove_char_json(ros_extract2.content)\n",
    "                ros_extract_json = json.loads(ros_extract2_corrected)\n",
    "            except:\n",
    "                # If it still fails, just give an empty JSON list\n",
    "                ros_extract_json = []\n",
    "        dflow[filename].append(ros_extract_json)\n",
    "\n",
    "dflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipe 4: LLM to identify the systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to clean up and organize ros entities captured from LLMs output: remove unnecessary characters and group them into ros category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def regexp_ros(text):\n",
    "  pattern = r\"-->\\s*(?P<ros>[^(\\n]+)\"\n",
    "  m = re.search(pattern, text, re.I)\n",
    "  if m:\n",
    "    return m.group('ros').strip()\n",
    "  else:\n",
    "    return 'None'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm2=ChatOllama(\n",
    "    model=\"ros_classify\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in dflow.keys():\n",
    "    if dflow[filename] != []:\n",
    "        ros_extracts = dflow[filename][2]\n",
    "        ai_output = \"\" # Track the AI output before regex process\n",
    "        for i in ros_extracts:\n",
    "            ros_cat = llm2.invoke(i['extract']).content\n",
    "            ai_output += f'###{ros_cat}\\n'\n",
    "            i['sys']=regexp_ros(ros_cat)\n",
    "        \n",
    "        dflow[filename].append(ai_output)\n",
    "dflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipe 5: Output to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{output_folder_path}/dflow.txt', 'w') as f:\n",
    "    f.write(str(dflow))\n",
    "\n",
    "with open(f'{output_folder_path}/results.csv', 'w') as f:\n",
    "    f.write(\"Filename,Extract,Status,Sys\\n\")\n",
    "    for filename in dflow.keys():\n",
    "        if dflow[filename] != []:\n",
    "            ros_results = dflow[filename][2]\n",
    "            if len(ros_results) == 0:\n",
    "                f.write(f\"{filename},NA,NA,NA\\n\")\n",
    "            else:\n",
    "                for i in ros_results:\n",
    "                    f.write(f\"{filename},{i['extract']},{i['status']},{i['sys']}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ros",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
